{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылка на опрос - https://goo.gl/forms/vO5Ls0eVJFdyhqL32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решающие деревья\n",
    "\n",
    "Решающее дерево — средство поддержки принятия решений, использующееся в статистике и анализе данных для прогнозных моделей. Структура дерева представляет собой «листья» и «ветки». На рёбрах («ветках») дерева решения записаны атрибуты, от которых зависит целевая функция, в «листьях» записаны значения целевой функции, а в остальных узлах — атрибуты. Чтобы классифицировать новый случай, надо спуститься по дереву до листа и выдать соответствующее значение. Подобные деревья решений широко используются в интеллектуальном анализе данных. Цель состоит в том, чтобы создать модель, которая предсказывает значение целевой переменной на основе нескольких переменных на входе.\n",
    "\n",
    "[title](img/CART_tree_titanic_survivors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.externals.six import StringIO\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# устанавливаем graphviz, pydot, pydotplus\n",
    "#!apt-get -qq install -y graphviz && pip install -q pydot && pip install -q pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pydotplus\n",
    "from IPython.core.display import display\n",
    "\n",
    "def draw_tree(dtree):\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(dtree, out_file=dot_data,\n",
    "                    filled=True, rounded=True,\n",
    "                    special_characters=True)\n",
    "\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "    display(Image(graph.create_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# загружаем датасет iris \n",
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# Разбиваем на обучающую и тестовую выборку: X_train, X_test, y_train, y_test \n",
    "# random_state=42, test_size=0.6\n",
    "# Здесь ваш код\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.6, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# строим дерево глубины 2, с критерием \"gini\", random_state=42\n",
    "# рисуем дерево с помощью функции draw_tree()\n",
    "# Здесь ваш код\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42, criterion='gini', max_depth=2)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred\n",
    "draw_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим верхнюю вершину дерева:\n",
    "\n",
    "$X_3 <= 1.65$ -- показвыает какой признак был выбран для разделения\n",
    "\n",
    "$gini = 0.655$ -- значение критерия gini\n",
    "\n",
    "$sample = 60$ -- количество сэмплов для разделения\n",
    "\n",
    "$value = [15, 21, 24]$ -- количество сэмплов каждого класса\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# посчитаем точность предсказания, с помощью accuracy_score, на test\n",
    "# Здесь ваш \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score = accuracy_score(y_pred, y_test)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# построим дерево с max depth 7, \"gini\", random_state 42 \n",
    "# какоме минимальное количество листьев max_leaf_nodes необходимо для ac >= 97%?\n",
    "# построим это дерево\n",
    "# Здесь ваш код\n",
    "clf = DecisionTreeClassifier(random_state=42, criterion='gini', max_depth=9)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred1 = clf.predict(X_test)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# посчитаем accuracy \n",
    "# Здесь ваш код\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score = accuracy_score(y_pred1, y_test)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайный лес\n",
    "\n",
    "Random forest (с англ. — «случайный лес») — алгоритм машинного обучения, предложенный Лео Брейманом и Адель Катлер, заключающийся в использовании комитета (ансамбля) решающих деревьев. Алгоритм сочетает в себе две основные идеи: метод бэггинга Бреймана, и метод случайных подпространств, предложенный Tin Kam Ho. Алгоритм применяется для задач классификации, регрессии и кластеризации. Основная идея заключается в использовании большого ансамбля решающих деревьев, каждое из которых само по себе даёт очень невысокое качество классификации, но за счёт их большого количества результат получается хорошим.\n",
    "\n",
    "\n",
    "Если вы обучите дерево решений на наборе данных, то оно сформирует некоторый набор правил, которые будут использоваться для прогнозов. Данное дерево может получиться глубоким и переобученным. Для сравнения, Random Forest случайным образом выбирает наблюдения и функции для построения нескольких деревьев решений, а затем усредняет их.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# построим композицию из 2-ух деревьев, с максимальным числом листьев - 7\n",
    "# random_state=42\n",
    "# Здесь ваш код\n",
    "\n",
    "# нарисуем два получившихся дерева\n",
    "# Здесь ваш код\n",
    "\n",
    "# построим композицию из 22-ти деревьев\n",
    "# Здесь ваш код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# посчитаем accuracy каждой \n",
    "# Здесь ваш код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Boosting\n",
    "\n",
    "Бустинг представляет собой жадный алгоритм построения композиции алгоритмов. Основная идея заключается в том, чтобы, имея множество относительно слабых алгоритмов обучения, построить их хорошую линейную комбинацию. Он похож на бэггинг тем, что базовый алгоритм обучения фиксирован. Отличие состоит в том, что обучение базовых алгоритмов для композиции происходит итеративно, и каждый следующий алгоритм стремится компенсировать недостатки композиции всех предыдущих алгоритмов.\n",
    "\n",
    "На примере бустинга стало ясно, что хорошим качеством могут обладать сколь угодно сложные композиции классификаторов, при условии, что они правильно настраиваются. Это развеяло существовавшие долгое время представления о том, что для повышения обобщающей способности необходимо ограничивать сложность алгоритмов. \n",
    "\n",
    "Впоследствии этот феномен бустинга получил теоретическое обоснование. Оказалось, что взвешенное голосование не увеличивает эффективную сложность алгоритма, а лишь сглаживает ответы базовых алгоритмов. Эффективность бустинга объясняется тем, что по мере добавления базовых алгоритмов увеличиваются отступы обучающих объектов. Причём бустинг продолжает раздвигать классы даже после достижения безошибочной классификации обучающей выборки.\n",
    "\n",
    "Общая схема бустинга:\n",
    "- Искомый ансамбль алгоритмов имеет вид $a(x) = \\mbox{sign}(\\sum_{t = 1}^T \\alpha_t b_t(x))$, где $b_t$ - базовые алгоритмы.\n",
    "- Ансамбль строится итеративно, оптимизируя на каждом шаге функционал $Q_t$, равный количеству ошибок текущей композиции на обучающей выборке.\n",
    "- При добавлении слагаемого $\\alpha_t b_t(x)$ в сумму, функционал $Q_t$ оптимизируется только по базовому алгоритму $b_t(x)$ и коэффициенту $\\alpha_t$ при нём, все предыдущие слагаемые считаются фиксированными.\n",
    "- Функционал $Q_t$ имеет вид суммы по объектам обучающей выборки пороговых функций вида $[y_i \\sum_{j = 1}^t \\alpha_j b_j(x_i) < 0]$, имеющих смысл \"текущая композиция ошибается на объекте с номером $i$\". Каждое такое слагаемое имеет вид \"ступеньки\" и является разрывной функцией. Для упрощения решения задачи оптимизации такая пороговая функция заменяется на непрерывно дифференцируемую оценку сверху. В итоге получается новый функционал $\\hat{Q}_t \\geqslant Q_t$, минимизация которого приводит к минимизации исходного функционала $Q_t$.\n",
    "\n",
    "Используя различные аппроксимации для пороговой функции потерь $[z < 0]$, будем получать различные виды бустинга. Примеры:\n",
    "- $e^{-z}$ - AdaBoost\n",
    "- $\\log_2(1 + e^{-z})$ - LogitBoost\n",
    "- $(1 - z)^2$ - GentleBoost\n",
    "- $e^{-cz(z+a)}$ - BrownBoost\n",
    "- другие"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# %pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-2, 2, 500)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x, x < 0, lw=2, label='Threshold function: $[z < 0$]')\n",
    "plt.plot(x, np.exp(-x), lw=2, label='AdaBoost')\n",
    "plt.plot(x, np.log2(1 + np.exp(-x)), lw=2, label='LogitBoost')\n",
    "plt.plot(x, (1 - x) ** 2, lw=2, label='GentleBoost')\n",
    "plt.plot(x, np.exp(-x * (x + 2)), lw=2, label='BrownBoost')\n",
    "plt.title('Various approximations of the threshold function')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм AdaBoost\n",
    "\n",
    "Как было сказано, алгоритм AdaBoost получается из описанной схемы, при аппроксимации пороговой функции потерь с помощью функции $e^{-z}$. Cуществует теорема (Freund, Schapire, 1996), дающая для достаточно богатых семейств базовых классификаторов явные формулы для базового алгоритма $b_t(x)$ и коэффициента $\\alpha_t$ при нём, на которых достигается минимум функционала $\\hat{Q}_t$. \n",
    "\n",
    "Сам алгоритм выглядит следующим образом:\n",
    "- Инициализировать веса объектов $w_i = \\frac{1}{l}, i = 1, \\dots, l$.\n",
    "- Для всех $t = 1, \\dots, T$\n",
    "    * Обучить базовый алгоритм $b_t = \\arg \\min_b N(b, W^l)$, где $N(b, W^l)$ есть взвешенная сумма ошибочных классификаций для $b_t$.\n",
    "    * $\\alpha_t = \\frac{1}{2}\\frac{1 - N(b_t, W^l)}{N(b_t, W^l)}$.\n",
    "    * Обновить веса объектов: $w_i = w_i e^{-\\alpha_t y_i b_t(x_i)}, i = 1, \\dots, l$.\n",
    "    * Нормировать веса объектов: $w_0 = \\sum_{j = 1}^k w_j, w_i = \\frac{w_i}{w_0}, i = 1, \\dots, l$.\n",
    "    \n",
    "Таким образом, вновь добавляемый алгоритм обучается путём минимизации взвешенной частоты ошибок на обучающей выборке, а не стандартного функционала, равного частоте ошибок. Вес объекта увеличивается в $e^{\\alpha_t}$ раз, когда $b_t$ допускает на нём ошибку, и уменьшается во столько же раз, когда $b_t$ правильно классифицирует этот объект. Таким образом, непосредственно перед настройкой базового алгоритма наибольший вес накапливается у тех объектов, которые чаще оказывались трудными для классификации предыдущими алгоритмами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting\n",
    "Метод градиентного бустинга в некотором смысле является обобщением остальных методов бустинга, поскольку он позволяет оптимизировать произвольную дифференцируемую функцию потерь. Данный алгоритм похож на метод градиентного спуска, применяемый для решения задач оптимизации. Основная идея заключается в том, что каждый следующий добавляемый в композицию алгоритм настраивается на остатки предыдущих алгоритмов.\n",
    "\n",
    "Пусть дана функция потерь дифференцируемая $L(F(x), y)$. Сам алгоритм выглядит следующим образом:\n",
    "- Инициализация композиции константным значением $F_0(x) = \\arg\\min_{\\alpha} \\sum_{i=1}^n L(\\alpha, y_i)$.\n",
    "- Для всех $t = 1, \\dots, T$:\n",
    "    * Вычислить остатки предыдущей композиции: $r_{it} = -[\\nabla_{F(x)} L(F(x_i), y_i)]_{F(x) = F_{t-1}(x)}, i = 1, \\dots, n$.\n",
    "    * Настроить базовый алгоритм $b_t(x)$ на полученные остатки, т.е. обучить его по выборке $\\{(x_i, r_{it}), i = 1, \\dots, n\\}$.\n",
    "    * Вычислить коэффициент $\\alpha_t$ перед базовым алгоритмом $b_t(x)$ как решение следующей одномерной задачи оптимизации:\n",
    "    $\\alpha_t = \\arg\\min_\\alpha \\sum_{i=1}^n L(F_{t-1}(x_i) + \\alpha b_t(x_i), y_i)$.\n",
    "    * Добавить полученное слагаемое в композицию: $F_t(x) = F_{t-1}(x) + \\alpha_t b_t(x)$.\n",
    "    \n",
    "Одной из возможных модификаций данного алгоритма является стохастический градиентный бустинг (SGB), который заключается в том, чтобы вычислять суммы вида $\\sum_{i=1}^n$ не по всей обучающей выборке, а только по некоторой её случайной подвыборке. Такой подход является одним из способов регуляризации данного алгоритма и позволяет улучшить качество композиции, сходимость алгоритма и время обучения. \n",
    "\n",
    "Другой способ регуляризации - это введение параметра $\\gamma$, называемого темпом обучения. При добавлении нового слагаемого в композицию, будем добавлять его, умноженное на этот коэффициент. Как правило, чем меньше темп обучения, тем лучше качество итоговой композиции.\n",
    "\n",
    "Для задач регресси обычно использую квадратичную функцию потерь $L(x, y) = (x - y)^2$ или модуль отклонения $L(x, y) = |x - y|$.\n",
    "В задаче классификации используется логистическая функция потерь, которая позволяет возвращать вероятности принадлежности объектов к классам.\n",
    "\n",
    "Одним из наиболее популярных семейств базовых алгоритмов являются решающие деревья. Именно такой вариант градиентного бустинга <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\">реализован</a> в sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры\n",
    "\n",
    "В sklearn доступны алгоритмы AdaBoost и GradientBoosting для задач классификации и регрессии.\n",
    "В качестве примера рассмотрим решение задачи восстановления одномерной регрессии с помощью <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\">GradientBoostingRegressor</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "     \n",
    "n_train = 150        \n",
    "n_test = 1000       \n",
    "noise = 0.1\n",
    "\n",
    "# Generate data\n",
    "def f(x):\n",
    "   # http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.ravel.html \n",
    "    x = x.ravel()\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "def generate(n_samples, noise):\n",
    "    X = np.random.rand(n_samples) * 10 - 5\n",
    "    X = np.sort(X).ravel()\n",
    "    y = np.exp(-X ** 2) + 1.5 * np.exp(-(X - 2) ** 2) +\\\n",
    "        np.random.normal(0.0, noise, n_samples)\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = generate(n_samples=n_train, noise=noise)\n",
    "X_test, y_test = generate(n_samples=n_test, noise=noise)\n",
    "# One decision tree regressor\n",
    "\n",
    "dtree = DecisionTreeRegressor(random_state=42)\n",
    "dtree = dtree.fit(X_train, y_train)\n",
    "y_pred = dtree.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "\n",
    "# Обучаем на train и делаем предсказание для X_test\n",
    "# Здесь ваш код \n",
    "\n",
    "# можно нарисовать дерево (оно - немаленькое)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# обучим RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# и GradientBoostingRegressor(n_estimators=100, subsample=0.5, random_state=42)  \n",
    "# Построим 3 графика, на которых посчитаем MSE\n",
    "# Здесь ваш код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_features=4, n_informative=2,\n",
    "                       random_state=0, shuffle=False)\n",
    "regr = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regr = regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, subsample=0.5, random_state=42)\n",
    "gbr = gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полезные ссылки\n",
    "- <a href=\"https://en.wikipedia.org/wiki/Boosting_(machine_learning)\">Boosting</a>\n",
    "- [Gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting)\n",
    "- [Лекция](http://www.machinelearning.ru/wiki/images/c/cd/Voron-ML-Compositions-slides.pdf) К.В. Воронцова по композиционным методам классификации\n",
    "- <a href=\"https://github.com/dmlc/xgboost\">Xgboost</a>\n",
    "- <a href=\"https://github.com/ChenglongChen/Kaggle_CrowdFlower\">Обзор</a> решения победителя соревнования Kaggle \"CrowdFlower\" по предсказанию релевантности выдачи поисковика товаров. Решение на основе Xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практика \n",
    "Один из самых важных параметров алгоритма бустинга является количество базовых моделей.<br\\>\n",
    "Слишком большое количество моделей может привести к переобучению, а слишком малое - к недообучению.<br\\>\n",
    "Как бы вы определяли оптимальное количество базовых моделей?\n",
    "\n",
    "Рассмотрите следующий california.dat. По этой таблице предлагается построить модель, предсказывающую стоимость дома в калифорнии по остальным признакам.\n",
    "\n",
    "* Загрузите данные и разбейте их на обучающую и контрольную выбору\n",
    "* Определите оптимальное количество базовых моделей в градиентном бустинге\n",
    "* Посмотрите, как ваш ответ меняется при изменении скорости обучения\n",
    "* В качестве ошибки используйте MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
